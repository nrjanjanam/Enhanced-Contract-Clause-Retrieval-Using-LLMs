{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem\n",
        "- Legal docs are 10-300 pages -  dense content , domain specific language\n",
        "- If you want to find clauses, right now people go through it manually\n",
        "- Takes hours per document minimum, can miss symantically similar words, synonyms and prone to human error\n",
        "Can we take document anaysis time from hours to seconds/ minutes?"
      ],
      "metadata": {
        "id": "ydqb-BnZKCPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assumptions\n",
        "- Documents - legal type(contracts)\n",
        "- Documents - digitized: Azure document intelligence\n",
        "- chunked - by page(each page is a chunk)\n",
        "- embed the page content (ADA-002)-  1500+ VECTOR OUTPUT -  Azure AI studio deployment\n",
        "- chunks are indexed along with document and chunk metadata - page number, document name, document type, document link, account, client in Azure AI Search\n",
        "\n"
      ],
      "metadata": {
        "id": "fPmrDnS09OAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and setup"
      ],
      "metadata": {
        "id": "WSKQyk6Zrepe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures\n",
        "from backend.utils import azure_utils as azure\n",
        "from azure.search.documents.models import VectorizedQuery\n",
        "from backend.utils import db_functions as db\n",
        "from backend.api.v1.main.models import *\n",
        "import numpy as np\n",
        "import json, tiktoken\n",
        "from math import ceil\n",
        "import logging\n",
        "import sys\n",
        "import traceback\n",
        "import time  # Added for implementing backoff"
      ],
      "metadata": {
        "id": "HBcpG4Iordg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(sys.stdout)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "rhnjTwsUrqB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Pipeline Code"
      ],
      "metadata": {
        "id": "15M-ZoSwrtEp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fj92U6agSQNn"
      },
      "outputs": [],
      "source": [
        "class LLMPipelineError(Exception):\n",
        "    \"\"\"Custom exception for LLM pipeline errors.\"\"\"\n",
        "    pass\n",
        "\n",
        "class LLMQueryPipeline:\n",
        "    def __init__(self, max_retries=3, backoff_factor=2):\n",
        "        \"\"\"\n",
        "        Initialize the LLMQueryPipeline.\n",
        "\n",
        "        :param max_retries: Maximum number of retry attempts for failed validations.\n",
        "        :param backoff_factor: Factor by which the wait time increases after each retry.\n",
        "        \"\"\"\n",
        "        self.max_retries = max_retries\n",
        "        self.backoff_factor = backoff_factor\n",
        "\n",
        "    def retrieve_document_chunks(self, document_id, max_results=10000):\n",
        "        \"\"\"\n",
        "        Fetch all chunks of a single document using pagination.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            search_client = azure.get_ai_search_client()\n",
        "            document_filters = f\"document_id eq '{document_id.upper()}'\"\n",
        "\n",
        "            batch_size = 100\n",
        "            total_batches = ceil(max_results / batch_size)\n",
        "            all_chunks = []\n",
        "            skip = 0\n",
        "\n",
        "            for batch in range(total_batches):\n",
        "                current_top = min(batch_size, max_results - len(all_chunks))\n",
        "                results = search_client.search(\n",
        "                    search_text=\"*\",  # Assuming you want all content; adjust as needed\n",
        "                    top=current_top,\n",
        "                    skip=skip,\n",
        "                    select=[\n",
        "                        \"account\",\n",
        "                        \"client_name\",\n",
        "                        \"page_number\",\n",
        "                        \"document_category\",\n",
        "                        \"document_title\",\n",
        "                        \"link\",\n",
        "                        \"content\",\n",
        "                        \"document_id\"\n",
        "                    ],\n",
        "                    filter=document_filters,\n",
        "                    semantic_configuration_name=\"my-semantic-config\",\n",
        "                    query_type=\"simple\",\n",
        "                    search_mode=\"any\",\n",
        "                )\n",
        "\n",
        "                batch_chunks = [result for result in results]\n",
        "                if not batch_chunks:\n",
        "                    break  # No more results\n",
        "                all_chunks.extend(batch_chunks)\n",
        "                skip += batch_size\n",
        "\n",
        "                if len(batch_chunks) < batch_size:\n",
        "                    break  # No more results\n",
        "\n",
        "            return all_chunks\n",
        "        except Exception as e:\n",
        "            logging.error(\"Error in retrieve_document_chunks: %s\", str(e))\n",
        "            logging.debug(traceback.format_exc())\n",
        "            raise LLMPipelineError(f\"Failed to retrieve chunks for document ID {document_id}.\") from e\n",
        "\n",
        "    def assemble_context(self, chunks):\n",
        "        \"\"\"\n",
        "        Convert fetched chunks into context format for the prompt.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            context = []\n",
        "            for chunk in chunks:\n",
        "                context.append({\n",
        "                    \"Account\": chunk.get(\"account\", \"N/A\"),\n",
        "                    \"Client Name\": chunk.get(\"client_name\", \"N/A\"),\n",
        "                    \"Page Number\": chunk.get(\"page_number\", \"N/A\"),\n",
        "                    \"Document Category\": chunk.get(\"document_category\", \"N/A\"),\n",
        "                    \"Document Title\": chunk.get(\"document_title\", \"N/A\"),\n",
        "                    \"Link\": chunk.get(\"link\", \"#\"),\n",
        "                    \"Content\": chunk.get(\"content\", \"\"),\n",
        "                })\n",
        "            return context\n",
        "        except Exception as e:\n",
        "            logging.error(\"Error in assemble_context: %s\", str(e))\n",
        "            logging.debug(traceback.format_exc())\n",
        "            raise LLMPipelineError(\"Failed to assemble context.\") from e\n",
        "\n",
        "    def split_context(self, context, max_tokens=100000):\n",
        "        \"\"\"\n",
        "        Split the context into smaller chunks each within the max_tokens limit.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "            current_chunk = []\n",
        "            current_tokens = 0\n",
        "            all_chunks = []\n",
        "\n",
        "            for entry in context:\n",
        "                entry_str = json.dumps(entry, indent=2)\n",
        "                entry_tokens = len(encoding.encode(entry_str))\n",
        "\n",
        "                if current_tokens + entry_tokens > max_tokens:\n",
        "                    if current_chunk:\n",
        "                        all_chunks.append(current_chunk)\n",
        "                        current_chunk = []\n",
        "                        current_tokens = 0\n",
        "\n",
        "                current_chunk.append(entry)\n",
        "                current_tokens += entry_tokens\n",
        "\n",
        "            if current_chunk:\n",
        "                all_chunks.append(current_chunk)\n",
        "\n",
        "            return all_chunks\n",
        "        except Exception as e:\n",
        "            logging.error(\"Error in split_context: %s\", str(e))\n",
        "            logging.debug(traceback.format_exc())\n",
        "            raise LLMPipelineError(\"Failed to split context into chunks.\") from e\n",
        "\n",
        "    def map_function(self, context_chunk, query, conversation_history):\n",
        "        \"\"\"\n",
        "        Generates a response for a context chunk and user query.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "            context_str = json.dumps(context_chunk, indent=2)\n",
        "            context_tokens = len(encoding.encode(context_str))\n",
        "\n",
        "            if context_tokens > 100000:\n",
        "                logging.error(\"Context chunk exceeds the maximum token limit.\")\n",
        "                raise LLMPipelineError(\"Context chunk is over 100000 tokens.\")\n",
        "\n",
        "            system_message = {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": '''You are an advanced AI assistant specialized in supporting\n",
        "                    the legal team with contract analysis. Your primary function\n",
        "                    is to help identify, extract, and summarize specific clauses or language\n",
        "                    within various types of contracts. Ensure all responses strictly adhere to the provided guidelines and formats.''',\n",
        "            }\n",
        "\n",
        "            context_message = {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Context (JSON format):\\n{context_str}\"\n",
        "            }\n",
        "\n",
        "            history_messages = [\n",
        "                {\"role\": msg[\"role\"], \"content\": msg[\"content\"]}\n",
        "                for msg in conversation_history\n",
        "            ]\n",
        "\n",
        "            user_query_message = {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"User's question: {query}\"\n",
        "            }\n",
        "\n",
        "            prompt_instructions = {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"\"\"Provide a concise answer based on the given context and conversation history.\n",
        "                The context is from a subset of a document. If there are no matches, simply return 'No matches found for the query' exactly.\n",
        "                If there are match(es):\n",
        "                - Always mention the page number the information comes from. Also identify the section of the document the citation is under and mention it in the response\n",
        "                - Cite the actual words from the document as well. Make sure there is enough context around the match in the citation\n",
        "                - Give a brief summary of the section the citation is from\n",
        "                - If a citation spans across multiple pages then always mention the page number as the lowest page where the citation starts from\n",
        "                - Use the following as an example output to ensure the formatting closely matches exactly like the example. Do not deviate from this format in any way:\n",
        "\n",
        "                Example Output:\n",
        "\n",
        "                1. **Page: page_number**\n",
        "                    - Under Section : Section Number and Section Heading\n",
        "                    - Section Summary: \"summary of the section the citation is derived from\"\n",
        "                    - Cited Text: \"content to be cited\"\n",
        "\n",
        "                2. **Page: page_number**\n",
        "                    - Under Section : Section Number and Section Heading\n",
        "                    - Section Summary: \"summary of the section the citation is derived from\"\n",
        "                    - Cited Text: \"content to be cited\"\n",
        "\n",
        "                Only provide the result in the given format. Do not hallucinate or use information that is not provided in the prompt.\"\"\"\n",
        "            }\n",
        "\n",
        "            messages = [system_message, context_message] + history_messages + [user_query_message, prompt_instructions]\n",
        "\n",
        "            openai_client, deployment_name = azure.get_gpt4o_client()\n",
        "\n",
        "            response = openai_client.chat.completions.create(\n",
        "                model=deployment_name,\n",
        "                messages=messages,\n",
        "                temperature=0.3,  # low temp for more deterministic output\n",
        "                stop=None,  # Define stop sequences if needed\n",
        "            )\n",
        "\n",
        "            response_content = response.choices[0].message.content.strip()\n",
        "\n",
        "            return response_content\n",
        "        except LLMPipelineError:\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            logging.error(\"Error in map_function: %s\", str(e))\n",
        "            logging.debug(traceback.format_exc())\n",
        "            raise LLMPipelineError(\"Failed to generate LLM response for the chunk.\") from e\n",
        "\n",
        "    def reduce_function(self, partial_responses, max_tokens=10000):\n",
        "        \"\"\"\n",
        "        Summarize the list of partial responses into a final coherent response.\n",
        "        If the combined responses exceed max_tokens, perform recursive summarization.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "            combined_responses = \"\\n\\n<subresponse>\".join(partial_responses)\n",
        "            combined_tokens = len(encoding.encode(combined_responses))\n",
        "\n",
        "            if combined_tokens <= max_tokens:\n",
        "                # Proceed to summarize\n",
        "                return self._generate_summary(combined_responses)\n",
        "            else:\n",
        "                # Split into smaller batches and summarize each batch first\n",
        "                batch_size = 10  # Adjust based on average response length to stay within limits\n",
        "                batched_responses = [partial_responses[i:i + batch_size] for i in range(0, len(partial_responses), batch_size)]\n",
        "                intermediate_summaries = []\n",
        "                for batch in batched_responses:\n",
        "                    intermediate_combined = \"\\n\\n<subresponse>\".join(batch)\n",
        "                    intermediate_summaries.append(self._generate_summary(intermediate_combined))\n",
        "                # Recursively summarize the intermediate summaries\n",
        "                return self.reduce_function(intermediate_summaries, max_tokens)\n",
        "        except Exception as e:\n",
        "            logging.error(\"Error in reduce_function: %s\", str(e))\n",
        "            logging.debug(traceback.format_exc())\n",
        "            raise LLMPipelineError(\"Failed to reduce partial responses.\") from e\n",
        "\n",
        "    def _generate_summary(self, combined_responses):\n",
        "        \"\"\"\n",
        "        Helper method to generate a summary from combined responses.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            system_message = {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": '''You are an advanced AI assistant specialized in summarizing information.\n",
        "                        Your task is to succinctly combine multiple summaries into a single coherent summary.\n",
        "                        Ensure strict adherence to the provided format and avoid any hallucinations.''',\n",
        "            }\n",
        "\n",
        "            user_message = {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"\n",
        "                Given the following summaries, generate a single concise and coherent summary. Each subquery response is separated by <subresponse> tags.\n",
        "                - Ignore any subquery that contains phrases like \"No matches from this subquery\" or similar.\n",
        "                - If all subqueries indicate no matches, respond with: \"No matches found for the query.\".\n",
        "                - Ensure the final summary strictly follows the specified format without deviations.\n",
        "\n",
        "                Format:\n",
        "\n",
        "                1. **Page: page_number**\n",
        "                    - Under Section : Section Number and Section Heading\n",
        "                    - Section Summary: \"summary of the section the citation is derived from\"\n",
        "                    - Cited Text: \"content to be cited\"\n",
        "\n",
        "                2. **Page: page_number**\n",
        "                    - Under Section : Section Number and Section Heading\n",
        "                    - Section Summary: \"summary of the section the citation is derived from\"\n",
        "                    - Cited Text: \"content to be cited\"\n",
        "\n",
        "                Summaries:\n",
        "                {combined_responses}\n",
        "                \"\"\"\n",
        "            }\n",
        "\n",
        "            openai_client, deployment_name = azure.get_gpt4o_client()\n",
        "\n",
        "            response = openai_client.chat.completions.create(\n",
        "                model=deployment_name,\n",
        "                messages=[system_message, user_message],\n",
        "                temperature=0,  # Set temperature to 0 for deterministic output\n",
        "                stop=None,  # Define stop sequences if needed\n",
        "            )\n",
        "\n",
        "            summary = response.choices[0].message.content.strip()\n",
        "\n",
        "            return summary\n",
        "        except Exception as e:\n",
        "            logging.error(\"Error in _generate_summary: %s\", str(e))\n",
        "            logging.debug(traceback.format_exc())\n",
        "            raise LLMPipelineError(\"Failed to generate summary.\") from e\n",
        "\n",
        "    def process_query(self, query, document_id, conversation_history):\n",
        "        \"\"\"Processes the query for a single document_id.\"\"\"\n",
        "        try:\n",
        "            if not document_id:\n",
        "                logging.info(\"No document ID provided.\")\n",
        "                return \"No matches found for the query\"\n",
        "\n",
        "            # Fetch all chunks of the specified document\n",
        "            chunks = self.retrieve_document_chunks(document_id, max_results=10000)  # Adjust as needed\n",
        "            if not chunks:\n",
        "                logging.info(f\"No chunks retrieved for document ID: {document_id}\")\n",
        "                return \"No matches found for the query\"\n",
        "\n",
        "            # Assemble context from chunks\n",
        "            context = self.assemble_context(chunks)\n",
        "            logging.info(f\"Number of context entries: {len(context)}\")\n",
        "\n",
        "            # Split context into manageable chunks for processing\n",
        "            context_chunks = self.split_context(context, max_tokens=100000)  # Adjust max_tokens as needed\n",
        "            logging.info(\"Number of context chunks: %d\", len(context_chunks))\n",
        "\n",
        "            partial_responses = []\n",
        "\n",
        "            # Define the number of worker threads; adjust as needed\n",
        "            max_workers = min(10, len(context_chunks))  # For example, up to 10 threads\n",
        "\n",
        "            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "                # Prepare the tasks\n",
        "                future_to_chunk = {\n",
        "                    executor.submit(\n",
        "                        self.map_function, chunk, query, conversation_history\n",
        "                    ): idx for idx, chunk in enumerate(context_chunks)\n",
        "                }\n",
        "\n",
        "                for future in concurrent.futures.as_completed(future_to_chunk):\n",
        "                    idx = future_to_chunk[future]\n",
        "                    try:\n",
        "                        partial_response = future.result()\n",
        "                        if partial_response:  # Ensure there is a response to summarize\n",
        "                            partial_responses.append(partial_response)\n",
        "                            logging.debug(f\"Partial response {idx + 1} added.\")\n",
        "                    except LLMPipelineError as e:\n",
        "                        logging.error(\"Error processing chunk %d: %s\", idx + 1, str(e))\n",
        "                        continue  # Skip failed chunks\n",
        "                    except Exception as e:\n",
        "                        logging.error(\"Unexpected error processing chunk %d: %s\", idx + 1, str(e))\n",
        "                        logging.debug(traceback.format_exc())\n",
        "                        continue  # Skip failed chunks\n",
        "\n",
        "            logging.info(\"Number of partial responses: %d\", len(partial_responses))\n",
        "\n",
        "            if not partial_responses:\n",
        "                return \"No matches found for the query\"\n",
        "\n",
        "            # Summarize all partial responses into a final response\n",
        "            final_response = self.reduce_function(partial_responses, max_tokens=10000)  # Adjust max_tokens as needed\n",
        "            return final_response\n",
        "\n",
        "        except LLMPipelineError:\n",
        "            return \"An error occurred while processing your query. Please try again later.\"\n",
        "        except Exception as e:\n",
        "            logging.error(\"Error in process_query: %s\", str(e))\n",
        "            logging.debug(traceback.format_exc())\n",
        "            return \"An unexpected error occurred. Please contact support.\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disclaimer:\n",
        "- No traiming possible on this datya: FORBIDDEN by the clients\n",
        "- No pattern in the documents ,each document is drafted by legal teram of client in multiple countries and can be in multiple languages.\n",
        "- No document standard format: could be column or 2 columns or 4 coulumns. Could be on letter grade, copuld be on A4 and there is 1 instance of it being written on a hankerchief\n",
        "- there is no trianing label"
      ],
      "metadata": {
        "id": "btQ2rjzvOPGx"
      }
    }
  ]
}